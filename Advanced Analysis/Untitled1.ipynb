{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ab7021-7650-46d5-857a-0b11bdd10294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import clone\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5905796a-ae9d-4967-9bf1-bdd37d84887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_set = pd.read_csv(\"train_cleandata.csv\")\n",
    "test_set = pd.read_csv(\"test_cleandata.csv\")\n",
    "\n",
    "# Convert categorical variables to categorical dtype\n",
    "categorical_cols = [\"quarter\", \"day\", \"team\", \"department\"]\n",
    "train_set[categorical_cols] = train_set[categorical_cols].astype(\"category\")\n",
    "test_set[categorical_cols] = test_set[categorical_cols].astype(\"category\")\n",
    "\n",
    "# Define predictors\n",
    "department_predictors = [\n",
    "    \"targeted_productivity\", \"smv\", \"over_time\", \"incentive\",\n",
    "    \"idle_time\", \"idle_men\", \"no_of_style_change\", \"no_of_workers\", \"team\"\n",
    "]\n",
    "\n",
    "overall_predictors = department_predictors + [\"department\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ed203c-79fa-49fa-beec-7950a228430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEPARTMENTS = {\n",
    "    \"Sewing\": department_predictors,\n",
    "    \"Finishing\": department_predictors,\n",
    "    \"Overall\": overall_predictors\n",
    "}\n",
    "\n",
    "OUTLIER_METHODS = {\n",
    "    \"isolation_forest\": IsolationForest(\n",
    "        contamination=0.1,\n",
    "        n_estimators=200,\n",
    "        max_features=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"none\": None\n",
    "}\n",
    "\n",
    "def detect_outliers(data, method):\n",
    "    \"\"\"Detect outliers using specified method\"\"\"\n",
    "    if method == \"none\":\n",
    "        return data\n",
    "    \n",
    "    detector = OUTLIER_METHODS[method]\n",
    "    numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "    detector.fit(data[numeric_cols])\n",
    "    inliers = detector.predict(data[numeric_cols]) == 1\n",
    "    return data[inliers]\n",
    "\n",
    "def prepare_data(train, test, predictors, outlier_method):\n",
    "    \"\"\"Preprocess data with specified outlier handling\"\"\"\n",
    "    # Detect and remove outliers\n",
    "    train_clean = detect_outliers(train, outlier_method)\n",
    "    test_clean = detect_outliers(test, outlier_method)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_clean[predictors]\n",
    "    X_test = test_clean[predictors]\n",
    "    y_train = train_clean[\"actual_productivity\"]\n",
    "    y_test = test_clean[\"actual_productivity\"]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Enhanced Parameter Grid with Pruning Focus\n",
    "RANDOM_FOREST_PARAMS = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [8, 10],\n",
    "    'min_samples_split': [15, 20],\n",
    "    'min_samples_leaf': [10, 15],\n",
    "    'max_features': [0.5, 0.6],\n",
    "    'max_leaf_nodes': [50, 100],\n",
    "    'ccp_alpha': np.linspace(0, 0.1, 20),\n",
    "    'max_samples': [0.7, 0.8]\n",
    "}\n",
    "\n",
    "def prune_forest(forest, X_train, y_train):\n",
    "    \"\"\"Advanced post-hoc pruning with cross-validated alpha selection\"\"\"\n",
    "    pruned_forest = clone(forest)\n",
    "    best_alphas = []\n",
    "    \n",
    "    # Prune each tree individually\n",
    "    for idx, estimator in enumerate(pruned_forest.estimators_):\n",
    "        # Get pruning path\n",
    "        path = estimator.cost_complexity_pruning_path(X_train, y_train)\n",
    "        ccp_alphas = path.ccp_alphas\n",
    "        \n",
    "        # Find optimal alpha using validation set\n",
    "        best_alpha = ccp_alphas[np.argmin(path.impurities)]\n",
    "        best_alphas.append(best_alpha)\n",
    "        \n",
    "        # Clone and prune tree\n",
    "        pruned_tree = DecisionTreeRegressor(random_state=42)\n",
    "        pruned_tree.set_params(**estimator.get_params())\n",
    "        pruned_tree.set_params(ccp_alpha=best_alpha)\n",
    "        pruned_tree.fit(X_train, y_train)\n",
    "        \n",
    "        pruned_forest.estimators_[idx] = pruned_tree\n",
    "    \n",
    "    return pruned_forest\n",
    "\n",
    "def train_rf_model(X_train, y_train):\n",
    "    \"\"\"Advanced training with two-stage tuning and pruning\"\"\"\n",
    "    # Stage 1: Initial parameter search\n",
    "    rf = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        oob_score=True,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        rf, RANDOM_FOREST_PARAMS,\n",
    "        n_iter=50,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        error_score='raise'\n",
    "    )\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    # Stage 2: Post-hoc pruning\n",
    "    best_model = search.best_estimator_\n",
    "    pruned_model = prune_forest(best_model, X_train, y_train)\n",
    "    \n",
    "    return pruned_model, search.best_params_\n",
    "\n",
    "def run_analysis(department, outlier_method):\n",
    "    \"\"\"Enhanced analysis pipeline with double regularization\"\"\"\n",
    "    # Data preparation\n",
    "    if department == \"Overall\":\n",
    "        train = train_set\n",
    "        test = test_set\n",
    "    else:\n",
    "        dept_lower = department.lower()\n",
    "        train = train_set[train_set[\"department\"] == dept_lower]\n",
    "        test = test_set[test_set[\"department\"] == dept_lower]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = prepare_data(\n",
    "        train, test, DEPARTMENTS[department], outlier_method\n",
    "    )\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    categorical_cols = X_train.select_dtypes(include=['category']).columns\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    # Transform data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Train model with pruning\n",
    "    final_model, best_params = train_rf_model(X_train_processed, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = {\n",
    "        \"train_mse\": mean_squared_error(y_train, final_model.predict(X_train_processed)),\n",
    "        \"test_mse\": mean_squared_error(y_test, final_model.predict(X_test_processed)),\n",
    "        \"train_r2\": r2_score(y_train, final_model.predict(X_train_processed)),\n",
    "        \"test_r2\": r2_score(y_test, final_model.predict(X_test_processed)),\n",
    "        \"oob_score\": final_model.oob_score_,\n",
    "        \"best_params\": best_params\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"department\": department,\n",
    "        \"outlier_method\": outlier_method,\n",
    "        **metrics\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550cbc74-6791-44c3-b06c-71b7bcf91c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressor' object has no attribute 'estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m finishing_no_outlier_result = \u001b[43mrun_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFinishing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43misolation_forest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mrun_analysis\u001b[39m\u001b[34m(department, outlier_method)\u001b[39m\n\u001b[32m    131\u001b[39m X_test_processed = preprocessor.transform(X_test)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Train model with pruning\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m final_model, best_params = \u001b[43mtrain_rf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    137\u001b[39m metrics = {\n\u001b[32m    138\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_mse\u001b[39m\u001b[33m\"\u001b[39m: mean_squared_error(y_train, final_model.predict(X_train_processed)),\n\u001b[32m    139\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtest_mse\u001b[39m\u001b[33m\"\u001b[39m: mean_squared_error(y_test, final_model.predict(X_test_processed)),\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_params\u001b[39m\u001b[33m\"\u001b[39m: best_params\n\u001b[32m    144\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mtrain_rf_model\u001b[39m\u001b[34m(X_train, y_train)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Stage 2: Post-hoc pruning\u001b[39;00m\n\u001b[32m    103\u001b[39m best_model = search.best_estimator_\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m pruned_model = \u001b[43mprune_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pruned_model, search.best_params_\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mprune_forest\u001b[39m\u001b[34m(forest, X_train, y_train)\u001b[39m\n\u001b[32m     61\u001b[39m best_alphas = []\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Prune each tree individually\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mpruned_forest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimators_\u001b[49m):\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Get pruning path\u001b[39;00m\n\u001b[32m     66\u001b[39m     path = estimator.cost_complexity_pruning_path(X_train, y_train)\n\u001b[32m     67\u001b[39m     ccp_alphas = path.ccp_alphas\n",
      "\u001b[31mAttributeError\u001b[39m: 'RandomForestRegressor' object has no attribute 'estimators_'"
     ]
    }
   ],
   "source": [
    "finishing_no_outlier_result = run_analysis(\"Finishing\", \"isolation_forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aab144-2163-4fd4-af4b-2f371e66479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "results = []\n",
    "for dept in [\"Sewing\", \"Finishing\", \"Overall\"]:\n",
    "    for method in [\"isolation_forest\", \"none\"]:\n",
    "        result = run_analysis(dept, method)\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[['department', 'outlier_method', 'train_r2', 'test_r2', 'oob_score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
